{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-21 00:08:05.023226: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-21 00:08:06.639772: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2023-04-21 00:08:06.640474: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1\n",
      "2023-04-21 00:08:07.038700: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1747] Found device 0 with properties: \n",
      "pciBusID: 0000:87:00.0 name: NVIDIA A100-SXM4-80GB computeCapability: 8.0\n",
      "coreClock: 1.41GHz coreCount: 108 deviceMemorySize: 79.18GiB deviceMemoryBandwidth: 1.85TiB/s\n",
      "2023-04-21 00:08:07.038742: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
      "2023-04-21 00:08:07.041952: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11\n",
      "2023-04-21 00:08:07.041989: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11\n",
      "2023-04-21 00:08:07.042744: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\n",
      "2023-04-21 00:08:07.042932: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n",
      "2023-04-21 00:08:07.043353: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.11\n",
      "2023-04-21 00:08:07.043979: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11\n",
      "2023-04-21 00:08:07.044080: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8\n",
      "2023-04-21 00:08:07.047642: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1889] Adding visible gpu devices: 0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras\n",
    "from tensorflow.keras import layers, models, losses, optimizers\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import class_weight\n",
    "import warnings\n",
    "# print(tf.config.list_physical_devices('GPU'))\n",
    "print(tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/xarray/backends/plugins.py:71: RuntimeWarning: Engine 'cfgrib' loading failed:\n",
      "Cannot find the ecCodes library\n",
      "  warnings.warn(f\"Engine {name!r} loading failed:\\n{ex}\", RuntimeWarning)\n"
     ]
    }
   ],
   "source": [
    "# import dataset\n",
    "datapath = \"../wildfire_dataset.nc\"\n",
    "wildfire_dataset = xr.open_dataset(datapath, engine=\"netcdf4\")\n",
    "# print(wildfire_dataset)\n",
    "feature_list = wildfire_dataset.data_vars\n",
    "feature_nums = len(feature_list)\n",
    "\n",
    "# maybe predict more than one thing later, but for not only try to predict burned areas\n",
    "remove_label = [\"burned_areas\", \"ignition_points\", \"number_of_fires\"]\n",
    "X_label = [label for label in feature_list if label not in remove_label]\n",
    "y_label = \"burned_areas\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take the first 5 time steps for all x and y to try creating a smaller dataset\n",
    "num_samples = 10\n",
    "timesteps_per_sample = 5\n",
    "timesteps = num_samples*timesteps_per_sample\n",
    "wf_dataset_head = wildfire_dataset.head(indexers={\"time\": timesteps})\n",
    "\n",
    "############# Normalize Data Here ####################\n",
    "\n",
    "wf_dataset_X = wf_dataset_head[X_label]\n",
    "wf_ds_norm_X = wf_dataset_X # REPLACE LATER WITH NORMALIZED/CORRECTLY STRUCTURED DATA\n",
    "\n",
    "wf_dataset_y = wf_dataset_head[y_label] \n",
    "wf_ds_norm_y = wf_dataset_y # REPLACE LATER WITH NORMALIZED/CORRECTLY STRUCTURED DATA\n",
    "\n",
    "######################################################\n",
    "\n",
    "# Create the y into a numpy matrix of shape (time, x, y)\n",
    "wf_dataset_y_np = wf_dataset_y.to_numpy()\n",
    "wf_dataset_y_np = np.transpose(wf_dataset_y_np, (0,2,1))\n",
    "\n",
    "# Create the X into a numpy matrix of shape (time, x, y)\n",
    "wf_dataset_X_np = wf_dataset_X[list(wf_dataset_X.data_vars)[0]].to_numpy()\n",
    "wf_dataset_X_np = np.transpose(wf_dataset_X_np, (0,2,1))\n",
    "wf_dataset_X_np = np.expand_dims(wf_dataset_X_np, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ndvi\n",
      "(50, 1253, 983, 1)\n",
      "evi\n",
      "(50, 1253, 983, 2)\n",
      "et\n",
      "(50, 1253, 983, 3)\n",
      "lst_day\n",
      "(50, 1253, 983, 4)\n",
      "lst_night\n",
      "(50, 1253, 983, 5)\n",
      "fapar\n",
      "(50, 1253, 983, 6)\n",
      "lai\n",
      "(50, 1253, 983, 7)\n",
      "max_u10\n",
      "(50, 1253, 983, 8)\n",
      "max_v10\n",
      "(50, 1253, 983, 9)\n",
      "max_d2m\n",
      "(50, 1253, 983, 10)\n",
      "max_t2m\n",
      "(50, 1253, 983, 11)\n",
      "max_sp\n",
      "(50, 1253, 983, 12)\n",
      "max_tp\n",
      "(50, 1253, 983, 13)\n",
      "min_u10\n",
      "(50, 1253, 983, 14)\n",
      "min_v10\n",
      "(50, 1253, 983, 15)\n",
      "min_d2m\n",
      "(50, 1253, 983, 16)\n",
      "min_t2m\n",
      "(50, 1253, 983, 17)\n",
      "min_sp\n",
      "(50, 1253, 983, 18)\n",
      "min_tp\n",
      "(50, 1253, 983, 19)\n",
      "avg_u10\n",
      "(50, 1253, 983, 20)\n",
      "avg_v10\n",
      "(50, 1253, 983, 21)\n",
      "avg_d2m\n",
      "(50, 1253, 983, 22)\n",
      "avg_t2m\n",
      "(50, 1253, 983, 23)\n",
      "avg_sp\n",
      "(50, 1253, 983, 24)\n",
      "avg_tp\n",
      "(50, 1253, 983, 25)\n",
      "smian\n",
      "(50, 1253, 983, 26)\n",
      "sminx\n",
      "(50, 1253, 983, 27)\n",
      "fwi\n",
      "(50, 1253, 983, 28)\n",
      "max_wind_u10\n",
      "(50, 1253, 983, 29)\n",
      "max_wind_v10\n",
      "(50, 1253, 983, 30)\n",
      "max_wind_speed\n",
      "(50, 1253, 983, 31)\n",
      "max_wind_direction\n",
      "(50, 1253, 983, 32)\n",
      "max_rh\n",
      "(50, 1253, 983, 33)\n",
      "min_rh\n",
      "(50, 1253, 983, 34)\n",
      "avg_rh\n",
      "(50, 1253, 983, 35)\n",
      "CLC_2006\n",
      "(50, 1253, 983, 36)\n",
      "CLC_2006_0\n",
      "(50, 1253, 983, 37)\n"
     ]
    }
   ],
   "source": [
    "# Takes each feature of the xarray Dataset and converts it into a DataArray \n",
    "# Also appends it into the new np array to make shape of (time x, y, features)\n",
    "for index, feature in enumerate(list(wf_dataset_X.data_vars)):\n",
    "    if(index!=0):\n",
    "        # Since wf_dataset_X_np is already initiaklized with the first element, skip\n",
    "        new_np_arr = wf_dataset_X[feature].to_numpy()\n",
    "        if(len(new_np_arr.shape) == 2):\n",
    "            # If a feature doesn't contain a time dimension (n), we extend the 2d matrix to 3d with copy of matrix n times\n",
    "            # Might be able to use numpy broadcast instead\n",
    "            new_np_arr = np.repeat(new_np_arr[:, :, np.newaxis], timesteps, axis=2)\n",
    "            # Transpose feature to \"time\", \"x\", \"y\" format\n",
    "            new_np_arr = np.transpose(new_np_arr)\n",
    "        else:\n",
    "            # Transpose feature to \"time\", \"x\", \"y\" format\n",
    "            new_np_arr = np.transpose(new_np_arr, (0,2,1))\n",
    "        if (np.isnan(new_np_arr).all()):\n",
    "            # Precaution to alert if a feature has all NaN values\n",
    "            warnings.warn(str(feature) + \" feature's values are all NaNs\")\n",
    "        wf_dataset_X_np = np.concatenate((wf_dataset_X_np, np.expand_dims(new_np_arr, axis=3)), axis=3)\n",
    "    print(feature)\n",
    "    print(wf_dataset_X_np.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rearrange so that it matches what keras expects (time, features, x, y) instead of (time, x, y, features)\n",
    "wf_dataset_X_np = np.moveaxis(wf_dataset_X_np, 3, 1)\n",
    "print(\"wf_dataset_X_np.shape\", wf_dataset_X_np.shape)\n",
    "print(\"wf_dataset_y_np.shape\", wf_dataset_y_np.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 1 in wf_dataset_y_np:\n",
    "    print(\"Fire exists\") \n",
    "print(\"Output classes: \", np.unique(wf_dataset_y_np))\n",
    "# class_weights = class_weight.compute_class_weight(class_weight = \"balanced\", classes = np.unique(wf_dataset_y_np), y = wf_dataset_y_np)\n",
    "# print(class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create samples (samples, time, features, x, y)\n",
    "# Each samples are 4 days with 2 day overlap between each one\n",
    "wf_dataset_X_np = np.expand_dims(wf_dataset_X_np, axis=0)\n",
    "wf_dataset_X_np = np.reshape(wf_dataset_X_np, (num_samples, timesteps_per_sample, wf_dataset_X_np.shape[2], wf_dataset_X_np.shape[3], wf_dataset_X_np.shape[4]))\n",
    "\n",
    "print(wf_dataset_X_np.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train test split (70/30 split)\n",
    "# split along axis 0\n",
    "wf_dataset_X_np_split = np.split(wf_dataset_X_np, [7, 10])\n",
    "wf_dataset_y_np_split = np.split(wf_dataset_y_np, [7, 10])\n",
    "X_train = wf_dataset_X_np_split[0]\n",
    "X_test = wf_dataset_X_np_split[1]\n",
    "y_train = wf_dataset_y_np_split[0]\n",
    "y_test = wf_dataset_y_np_split[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"X_train: \", X_train.shape)\n",
    "print(\"X_test: \", X_test.shape)\n",
    "print(\"y_train: \", y_train.shape)\n",
    "print(\"y_train: \", y_train.dtype)\n",
    "print(\"y_test: \", y_test.shape)\n",
    "print(\"input shape: \", X_train.shape[-4:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
